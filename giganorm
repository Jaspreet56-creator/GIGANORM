#!/usr/bin/env bash

cat <<'EOF'
                                                ############################################################################
                                                #                                                                          #
                                                #     ██████╗ ██╗ ██████╗  █████╗ ███╗   ██╗ ██████╗ ██████╗ ███╗   ███╗   #
                                                #    ██╔════╝ ██║██╔════╝ ██╔══██╗████╗  ██║██╔═══██╗██╔══██╗████╗ ████║   #
                                                #    ██║  ███╗██║██║  ███╗███████║██╔██╗ ██║██║   ██║██████╔╝██╔████╔██║   #
                                                #    ██║   ██║██║██║   ██║██╔══██║██║╚██╗██║██║   ██║██╔══██╗██║╚██╔╝██║   #
                                                #    ╚██████╔╝██║╚██████╔╝██║  ██║██║ ╚████║╚██████╔╝██║  ██║██║ ╚═╝ ██║   #
                                                #     ╚═════╝ ╚═╝ ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═══╝ ╚═════╝ ╚═╝  ╚═╝╚═╝     ╚═╝   #
                                                #                                                                          #
                                                #           GIGANORM:   When your datasets are so big,                     #
                                                #                       only a pipeline with 'giga' energy can             #
                                                #                       chew through them (and burp up a matrix).          #
                                                #                                                                          #
                                                #                         "Go big, get normalized."                        #
                                                #                                                                          #
                                                ############################################################################
EOF

set -euo pipefail

show_help() {
  local prog
  prog=$(basename "$0")
  cat <<EOF
Usage:
  $prog <srr_list.txt> <coords.bed> <STAR_index_dir> <output_dir> <batch_size> <metric>
  $prog [--keep-intermediate] <srr_list.txt> <coords.bed> <STAR_index_dir> <output_dir> <batch_size> <metric>
  $prog [--keep-intermediate] [--input_fastq] <fastq_files.txt> <coords.bed> <STAR_index_dir> <output_dir> <batch_size> <metric>
  $prog --local <srr_list.txt> <coords.bed> <STAR_index_dir> <output_dir> <metric>
  $prog --local [--input_fastq] <fastq_files.txt> <coords.bed> <STAR_index_dir> <output_dir> <metric>
  $prog --local [--keep-intermediate] [--input_fastq] <fastq_files.txt> <coords.bed> <STAR_index_dir> <output_dir> <metric>

Arguments:
  srr_list.txt     File with one SRR accession per line.
  fastq_files.txt  (with --input_fastq) List of FASTQ file paths: single-end (one per line) or paired-end ("fq1::fq2" per line).
  coords.bed       BED file of regions.
  STAR_index_dir   Path to STAR genome index directory.
  output_dir       Root directory for outputs.
  batch_size       Max concurrent tasks (ONLY FOR SLURM MODE).
  metric           TPM, RPKM, or FPKM.

Options:
  --input_fastq        Use a FASTQ file list instead of SRR list.
  --keep-intermediate  Keep all intermediate files.
  --local              Run all samples serially (no SLURM; for both SRR and FASTQ).
  -h, --help           Show this help message and exit.
EOF
}




KEEP_INTERMEDIATE=0
MODE="pipeline"
INPUT_MODE="srr"  # Default
EXTRA_FLAGS=()

# --- Flag parsing with propagation ---
while [[ $# -gt 0 ]]; do
  case "$1" in
    --input_fastq)
      INPUT_MODE="fastq"
      EXTRA_FLAGS+=("--input_fastq")
      shift
      ;;
    --keep|--keep-intermediate)
      KEEP_INTERMEDIATE=1
      EXTRA_FLAGS+=("--keep-intermediate")
      shift
      ;;
    --run-task)
      MODE="run-task"
      shift
      ;;
    --merge-matrix)
      MODE="merge-matrix"
      shift
      ;;
    --local)
      MODE="local"
      EXTRA_FLAGS+=("--local")
      shift
      ;;
    -h|--help)
      show_help
      exit 0
      ;;
    *)
      break   # End of flags; positional args start here
      ;;
  esac
done

if [[ $# -lt 4 ]]; then
  echo "Error: Not enough arguments supplied."
  show_help
  exit 1
fi
if [[ "$INPUT_MODE" == "fastq" ]]; then
    fastq_list="$1"; shift
    srr_list=""
else
    srr_list="$1"; shift
    fastq_list=""
fi

coords_bed="$1"; shift
STAR_index_dir="$1"; shift
output_dir="$1"; shift

if [[ "$MODE" == "pipeline" ]]; then
    if [[ $# -lt 2 ]]; then
      echo "Error: batch_size and metric must be supplied for pipeline mode."
      show_help
      exit 1
    fi
    batch_size="$1"; shift
    metric="${1^^}"; shift
else
    metric="${1^^}"; shift
fi

coords_bed=$(readlink -f "$coords_bed")
STAR_index_dir=$(readlink -f "$STAR_index_dir")
output_dir=$(readlink -f "$output_dir")
if [[ "$INPUT_MODE" == "fastq" ]]; then
    fastq_list=$(readlink -f "$fastq_list")
else
    srr_list=$(readlink -f "$srr_list")
fi

if [[ "$metric" != "TPM" && "$metric" != "RPKM" && "$metric" != "FPKM" ]]; then
  echo "Error: Metric must be one of TPM, RPKM, FPKM" >&2
  exit 1
fi

BED_FIXED=0
coords_bed_fixed="${coords_bed}.tabfix"
if [[ -f "$coords_bed" ]] && ! head -n 5 "$coords_bed" | grep -q $'\t'; then
    awk '{$1=$1}1' OFS='\t' "$coords_bed" > "$coords_bed_fixed"
    if [[ $? -eq 0 ]]; then
      echo "[$(date)] [BED FIX] Converted '$coords_bed' to tab-delimited BED as '$coords_bed_fixed'"
      coords_bed="$coords_bed_fixed"
      BED_FIXED=1
    else
      echo "[$(date)] [BED FIX] ERROR: Failed to convert '$coords_bed' to tab-delimited!"
      exit 98
    fi
fi

if [[ "$MODE" == "local" ]]; then
  if [[ "$INPUT_MODE" == "fastq" ]]; then
      total=$(grep -cve '^\s*$' "$fastq_list")
      for tid in $(seq 1 "$total"); do
        echo "========= Running sample $tid/$total ==========="
        SLURM_ARRAY_TASK_ID=$tid "$0" "${EXTRA_FLAGS[@]}" --run-task "$fastq_list" "$coords_bed" "$STAR_index_dir" "$output_dir" "$metric"
      done
  else
      total=$(wc -l < "$srr_list")
      for tid in $(seq 1 "$total"); do
        echo "========= Running sample $tid/$total ==========="
        SLURM_ARRAY_TASK_ID=$tid "$0" "${EXTRA_FLAGS[@]}" --run-task "$srr_list" "$coords_bed" "$STAR_index_dir" "$output_dir" "$metric"
      done
  fi
  exit 0
fi

if [[ "$MODE" == "pipeline" ]]; then
  if [[ "$INPUT_MODE" == "fastq" ]]; then
    N=$(grep -cve '^\s*$' "$fastq_list")
    job_name="giganorm"
    mkdir -p "${output_dir}/logs" "${output_dir}/fastq" "${output_dir}/bam" \
      "${output_dir}/counts" "${output_dir}/logs" "${output_dir}/${metric}"
    sbatch_args=(--array=1-"$N"%$batch_size
      --job-name="$job_name"
      --cpus-per-task=8
      --mem-per-cpu=12G
      --export=ALL
      --time=01:30:00
      --ntasks=1
      --output="${output_dir}/logs/%A_%a.out"
      --error="${output_dir}/logs/%A_%a.err"
      --chdir="${output_dir}"
      "$0"
    )
    [[ "$KEEP_INTERMEDIATE" -eq 1 ]] && sbatch_args+=(--keep-intermediate)
    sbatch_args+=(--input_fastq --run-task "$fastq_list" "$coords_bed" "$STAR_index_dir" "$output_dir" "$metric")
    exec sbatch "${sbatch_args[@]}"
    exit 0
  else
    N=$(wc -l < "$srr_list")
    job_name="giganorm"
    mkdir -p "${output_dir}/logs" "${output_dir}/fastq" "${output_dir}/bam" \
      "${output_dir}/counts" "${output_dir}/logs" "${output_dir}/${metric}"
    sbatch_args=(--array=1-"$N"%$batch_size
      --job-name="$job_name"
      --cpus-per-task=8
      --mem-per-cpu=12G
      --export=ALL
      --time=01:30:00
      --ntasks=1
      --output="${output_dir}/logs/%A_%a.out"
      --error="${output_dir}/logs/%A_%a.err"
      --chdir="${output_dir}"
      "$0"
    )
    [[ "$KEEP_INTERMEDIATE" -eq 1 ]] && sbatch_args+=(--keep-intermediate)
    sbatch_args+=(--run-task "$srr_list" "$coords_bed" "$STAR_index_dir" "$output_dir" "$metric")
    exec sbatch "${sbatch_args[@]}"
    exit 0
  fi
fi

# =========================
#      TASK MODE
# =========================

module load WebProxy

task_id="${SLURM_ARRAY_TASK_ID:-1}"
if [[ "$INPUT_MODE" == "fastq" ]]; then
    fq_line=$(awk "NR==$task_id" "$fastq_list")
    fq1=""; fq2=""
    if [[ "$fq_line" == *::* ]]; then
        fq1="${fq_line%%::*}"
        fq2="${fq_line##*::}"
        base=$(basename "$fq1")
        SAMPLE="${base%_1.fastq.gz}"
    else
        fq1="$fq_line"
        fq2=""
        base=$(basename "$fq1")
        SAMPLE="${base%.fastq.gz}"
    fi
    SRR="$SAMPLE"
else
    SRR=$(sed -n "${task_id}p" "$srr_list" | tr -d '\r\n')
fi

if [[ -z "$SRR" ]]; then echo "No SRR for task $task_id"; exit 2; fi

logfile="${output_dir}/logs/${task_id}.log"
exec > >(tee -a "$logfile") 2>&1

echo "[$(date)] Starting SRR $SRR (task $task_id)"
for subdir in fastq bam counts logs "$metric" fastqc; do mkdir -p "${output_dir}/$subdir"; done
cd "$output_dir"

# --- FASTP + FASTQC helper functions ---
trim_fastqs() {
    local fq1="$1"
    local fq2="$2"
    local out1="$3"
    local out2="$4"
    local sample="$5"
    local jsonlog="fastq/${sample}_fastp.json"
    local htmlrep="fastq/${sample}_fastp.html"
    if [[ -n "$fq2" ]]; then
        echo "[$(date)] [FASTP] Trimming paired-end $fq1 $fq2"
        fastp -i "$fq1" -I "$fq2" -o "$out1" -O "$out2" -w 4 -j "$jsonlog" -h "$htmlrep"
        ret=$?
    else
        echo "[$(date)] [FASTP] Trimming single-end $fq1"
        fastp -i "$fq1" -o "$out1" -w 4 -j "$jsonlog" -h "$htmlrep"
        ret=$?
    fi
    return $ret
}
run_fastqc() {
    mkdir -p "${output_dir}/fastqc"
    echo "[$(date)] [FASTQC] Running FastQC on $*"
    fastqc -o "${output_dir}/fastqc" -q "$@"
}
TO_DELETE=()

# ---- FASTQ/FASTP/FASTQC block (all input types) ----
if [[ "$INPUT_MODE" == "fastq" ]]; then
    if [[ -n "${fq2:-}" ]]; then
        fq1_trim="fastq/${SAMPLE}_1.trimmed.fastq.gz"
        fq2_trim="fastq/${SAMPLE}_2.trimmed.fastq.gz"
        trim_fastqs "$fq1" "$fq2" "$fq1_trim" "$fq2_trim" "$SAMPLE" || { echo "[fastp] trimming failed for $SAMPLE"; exit 14; }
        run_fastqc "$fq1_trim" "$fq2_trim"
        READS="$fq1_trim $fq2_trim"
        TO_DELETE+=("$fq1" "$fq2" "$fq1_trim" "$fq2_trim" "fastq/${SAMPLE}_fastp.json" "fastq/${SAMPLE}_fastp.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq2_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq2_trim%.gz}")_fastqc.html")
    else
        fq1_trim="fastq/${SAMPLE}.trimmed.fastq.gz"
        trim_fastqs "$fq1" "" "$fq1_trim" "" "$SAMPLE" || { echo "[fastp] trimming failed for $SAMPLE"; exit 14; }
        run_fastqc "$fq1_trim"
        READS="$fq1_trim"
        TO_DELETE+=("$fq1" "$fq1_trim" "fastq/${SAMPLE}_fastp.json" "fastq/${SAMPLE}_fastp.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.html")
    fi
    echo "[$(date)] [FASTQ INPUT] Using trimmed FASTQ(s): $READS"
else

    if [[ -e "fastq/${SRR}_1.fastq.gz" && -e "fastq/${SRR}_2.fastq.gz" ]] || \
       [[ -e "fastq/${SRR}.fastq.gz" ]] || \
       [[ -e "fastq/${SRR}_1.fastq.gz" ]]; then
      echo "[$(date)] [Step 1] FASTQ already exists for $SRR, skipping download/extraction."
    else
      
      echo "[$(date)] [Step 1] Downloading $SRR with prefetch"
      
    if ! prefetch "$SRR"; then
      echo "[$(date)] [ERROR] prefetch failed for $SRR"; exit 10
    fi
    sra_file=""
    if [[ -f "${output_dir}/${SRR}/${SRR}.sra" ]]; then
      sra_file="${output_dir}/${SRR}/${SRR}.sra"
    elif [[ -f "$HOME/ncbi/public/sra/${SRR}.sra" ]]; then
      sra_file="$HOME/ncbi/public/sra/${SRR}.sra"
    else
      found=$(find . "$HOME/ncbi/public/sra" -type f -name "${SRR}.sra" | head -n 1)
      [[ -n "$found" && -f "$found" ]] && sra_file="$found"
    fi
    if [[ -z "$sra_file" || ! -f "$sra_file" ]]; then
      echo "[$(date)] [ERROR] SRA file not found after prefetch."
      exit 11
    fi
    echo "[$(date)] SRA file path: $sra_file"
    echo "[$(date)] [Step 3] Extracting FASTQ from $sra_file"
    if ! fasterq-dump \
          --split-3 \
          --threads "${SLURM_CPUS_PER_TASK:-8}" \
          --outdir fastq \
          "$sra_file"; then
      echo "[$(date)] [ERROR] fasterq-dump failed on $sra_file, trying ENA fallback"
      ena_base="https://ftp.sra.ebi.ac.uk/vol1/fastq/${SRR:0:6}/${SRR}"
      err_count=0
      for suffix in "" "_1" "_2"; do
        f="fastq/${SRR}${suffix}.fastq.gz"
        url="${ena_base}/${SRR}${suffix}.fastq.gz"
        if [[ ! -e "$f" ]]; then
          if ! wget -O "$f" "$url"; then
            echo "[$(date)] [ERROR] ENA wget failed for $url"
            ((err_count++))
          fi
        fi
      done
      if (( err_count == 3 )); then
        echo "[$(date)] [ERROR] Could not get FASTQs from SRA or ENA for $SRR"
        exit 12
      fi
    else
      echo "[$(date)] Compressing with pigz"
      pigz -p "${SLURM_CPUS_PER_TASK:-8}" fastq/"${SRR}"*.fastq
      echo "[$(date)] Extraction and compression complete"
      rm -f "$sra_file"
    fi
    fi

    if [[ -e "fastq/${SRR}_1.fastq.gz" && -e "fastq/${SRR}_2.fastq.gz" ]]; then
        fq1="fastq/${SRR}_1.fastq.gz"
        fq2="fastq/${SRR}_2.fastq.gz"
        fq1_trim="fastq/${SRR}_1.trimmed.fastq.gz"
        fq2_trim="fastq/${SRR}_2.trimmed.fastq.gz"
        trim_fastqs "$fq1" "$fq2" "$fq1_trim" "$fq2_trim" "$SRR" || { echo "[fastp] trimming failed for $SRR"; exit 14; }
        run_fastqc "$fq1_trim" "$fq2_trim"
        READS="$fq1_trim $fq2_trim"
        TO_DELETE+=("$fq1" "$fq2" "$fq1_trim" "$fq2_trim" "fastq/${SRR}_fastp.json" "fastq/${SRR}_fastp.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq2_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq2_trim%.gz}")_fastqc.html")
    elif [[ -e "fastq/${SRR}.fastq.gz" ]]; then
        fq1="fastq/${SRR}.fastq.gz"
        fq1_trim="fastq/${SRR}.trimmed.fastq.gz"
        trim_fastqs "$fq1" "" "$fq1_trim" "" "$SRR" || { echo "[fastp] trimming failed for $SRR"; exit 14; }
        run_fastqc "$fq1_trim"
        READS="$fq1_trim"
        TO_DELETE+=("$fq1" "$fq1_trim" "fastq/${SRR}_fastp.json" "fastq/${SRR}_fastp.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.html")
    elif [[ -e "fastq/${SRR}_1.fastq.gz" ]]; then
        fq1="fastq/${SRR}_1.fastq.gz"
        fq1_trim="fastq/${SRR}_1.trimmed.fastq.gz"
        trim_fastqs "$fq1" "" "$fq1_trim" "" "$SRR" || { echo "[fastp] trimming failed for $SRR"; exit 14; }
        run_fastqc "$fq1_trim"
        READS="$fq1_trim"
        TO_DELETE+=("$fq1" "$fq1_trim" "fastq/${SRR}_fastp.json" "fastq/${SRR}_fastp.html")
        TO_DELETE+=("${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.zip" "${output_dir}/fastqc/$(basename "${fq1_trim%.gz}")_fastqc.html")
    else
      echo "[$(date)] [ERROR] No FASTQ found for $SRR after all download attempts"; exit 13
    fi
    echo "[$(date)] [FASTQ INPUT] Using trimmed FASTQ(s): $READS"
fi


# STAR Alignment (skip if BAM exists) 
in_bam="bam/${SRR}.Aligned.out.bam"
if [[ -e "$in_bam" ]]; then
  echo "[$(date)] [Step 5] BAM already exists ($in_bam), skipping alignment."
else
  echo "[$(date)] [Step 5] Aligning with STAR"
  if ! STAR \
    --runThreadN "${SLURM_CPUS_PER_TASK:-8}" \
    --genomeDir "$STAR_index_dir" \
    --readFilesIn $READS \
    --readFilesCommand zcat \
    --outFileNamePrefix "bam/${SRR}." \
    --outSAMtype BAM Unsorted \
    --outFilterMultimapNmax 1; then
    echo "[$(date)] [ERROR] Alignment failed for $SRR"; exit 20
  fi
fi

#  Unique filtering 
echo "[$(date)] [Step 6] Filtering uniquely mapped reads (MAPQ=255)"
tmp_bam="bam/${SRR}.tmp.bam"
if ! samtools view -b -@ "${SLURM_CPUS_PER_TASK:-8}" -q 255 "$in_bam" > "$tmp_bam"; then
  echo "[$(date)] [ERROR] samtools filtering failed for $SRR"; exit 21
fi
mv "$tmp_bam" "$in_bam"

#  Library Size 
echo "[$(date)] [Step 8] Counting library size"
libsize=$(samtools view -c "$in_bam")
if [[ -z "$libsize" || "$libsize" -eq 0 ]]; then
  echo "[$(date)] [ERROR] Failed to get library size for $SRR"; exit 23
fi

#  bedtools coverage 
echo "[$(date)] [Step 9] Calculating coverage over regions"
counts_file="counts/${SRR}.counts.txt"
if ! bedtools coverage -a "$coords_bed" -b "$in_bam" -counts > "$counts_file"; then
  echo "[$(date)] [ERROR] bedtools coverage failed for $SRR"; exit 24
fi

#  Metric calculation 
echo "[$(date)] [Step 10] Calculating $metric"
metric_out="${metric}/${SRR}.${metric}.txt"
if ! awk -v OFS="\t" -v METRIC="$metric" -v libsize="$libsize" '
  {
    key = $1":"$2"-"$3;
    raw_count = $5;
    len = $3 - $2;
    rpk = raw_count/(len/1000);
    norm = libsize/1e6;
    final = rpk/norm;
    print $1, $2, $3, $4, raw_count, len, rpk, final
  }
' "$counts_file" | awk -v OFS="\t" -v metric="$metric" 'BEGIN{print "chr","start","end","region","raw_count","length_bp","rpk",metric}{print $0}' > "$metric_out"; then
  echo "[$(date)] [ERROR] Metric calculation failed for $SRR"; exit 25
fi


if [[ "$KEEP_INTERMEDIATE" -eq 0 ]]; then
  echo "[$(date)] [Step 11] Cleaning up intermediate files for $SRR (KEEP_INTERMEDIATE=0)"
  rm -f fastq/${SRR}*.fastq.gz bam/${SRR}* counts/${SRR}*
  if [[ -d "${SRR}" ]]; then
    rm -rf "${SRR}"
    echo "[$(date)]   Deleted ${SRR} (SRA dir)"
  fi
  for d in bam counts; do
    [[ -d "${d}/${SRR}" ]] && rm -rf "${d:?}/${SRR}"
  done
  [[ "$BED_FIXED" -eq 1 ]] && rm -f "$coords_bed_fixed"
else
  echo "[$(date)] [Step 11] Keeping intermediate files and directories as requested (KEEP_INTERMEDIATE=1)."
fi

echo "[$(date)] Done for $SRR"

# .TPM.txt files into a combined matrix 
metric_dir="${output_dir}/${metric}"
matrix_out="${metric_dir}/combined_${metric}_matrix.tsv"
TPM_FILES=($(ls "${metric_dir}"/*.${metric}.txt 2>/dev/null | sort))
NFILES=${#TPM_FILES[@]}

if [[ "$INPUT_MODE" == "fastq" ]]; then
  expected=$(grep -cve '^\s*$' "$fastq_list")
else
  expected=$(wc -l < "$srr_list")
fi

if [[ $NFILES -eq $expected ]]; then
  {
    printf "sample"
    awk 'NR>1 { printf "\t%s:%s-%s", $1, $2, $3 }' "${TPM_FILES[0]}"
    printf "\n"
  } > "$matrix_out"
  for f in "${TPM_FILES[@]}"; do
    sample=$(basename "$f" | cut -d. -f1)
    printf "%s" "$sample" >> "$matrix_out"
    awk 'NR>1 { printf "\t%s", $8 } END{ print "" }' "$f" >> "$matrix_out"
  done
  echo "[$(date)] [GIGANORM] Merged all sample metrics to: $matrix_out"
fi
